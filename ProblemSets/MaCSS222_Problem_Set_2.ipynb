{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d9897c3",
   "metadata": {},
   "source": [
    "## Problem Set 2\n",
    "_MaCSS 222 Applied Statistics II_\n",
    "_Spring 2025_\n",
    "\n",
    "This Problem Set will, like the first problem set utilize an extract from the 1997 cohort of the National Longitudinal Survey of Youth (NLSY97). This Notebook is designed to help you get started. Serving as a stub for your own solutions, completing the problem set will involve adding code and commentary to this notebook. Narrative answers to questions posed in the Problem Set can be included in markdown boxes in this notebook. For \"pencil and paper\" calculations you can either hand-write your answers and turn in a pdf-scan of them along with your Python Jupyter Notebook, or you can write you answers in LaTex in markdown boxes. Please see the pdf file for Problem Set 1 for instructions.\n",
    "<br>\n",
    "<br>\n",
    "If you are not already familiar with LaTex, I encourage you to learn the basics. Overleaf is a helpful online editing environment for LaTex which you can access as a UC Berkeley community member."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97df37a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9d9ef8",
   "metadata": {},
   "source": [
    "### Part I: Load NLSY97 extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c7db6355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where NLSY97 teaching extract file is located\n",
    "data =  '/Users/bgraham/Dropbox/Teaching/Berkeley_Courses/MaCSS/Data/'\n",
    "\n",
    "# Directory to save graphics files in\n",
    "graphics = '/Users/bgraham/Dropbox/Teaching/Berkeley_Courses/MaCSS/Graphics/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f12b81cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load NLSY97 dataset\n",
    "nlsy97 = pd.read_csv(data+'nlsy97ss.csv') # Reading .csv as DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d8142ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename some columns and then form new dataframe with complete cases for key variables\n",
    "nlsy97.rename(columns={'avg_earn_2016_to_2020': 'earnings', 'hgc_at_age28': 'yrssch'}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e89e9f0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hhid97</th>\n",
       "      <th>pid97</th>\n",
       "      <th>swgt</th>\n",
       "      <th>cs_smpl</th>\n",
       "      <th>earnings</th>\n",
       "      <th>yrssch</th>\n",
       "      <th>asvab</th>\n",
       "      <th>female</th>\n",
       "      <th>black</th>\n",
       "      <th>hispanic</th>\n",
       "      <th>birth_month</th>\n",
       "      <th>birth_year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>272178</td>\n",
       "      <td>1</td>\n",
       "      <td>140764.964069</td>\n",
       "      <td>14.0</td>\n",
       "      <td>58.483</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>July</td>\n",
       "      <td>1982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>169357</td>\n",
       "      <td>1</td>\n",
       "      <td>34487.266323</td>\n",
       "      <td>14.0</td>\n",
       "      <td>27.978</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>September</td>\n",
       "      <td>1983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>149099</td>\n",
       "      <td>1</td>\n",
       "      <td>45864.738658</td>\n",
       "      <td>13.0</td>\n",
       "      <td>37.012</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>February</td>\n",
       "      <td>1981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>218371</td>\n",
       "      <td>1</td>\n",
       "      <td>23202.102210</td>\n",
       "      <td>14.0</td>\n",
       "      <td>22.001</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>January</td>\n",
       "      <td>1982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>232055</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.0</td>\n",
       "      <td>3.585</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>April</td>\n",
       "      <td>1983</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   hhid97  pid97    swgt  cs_smpl       earnings  yrssch   asvab  female  \\\n",
       "0       2      2  272178        1  140764.964069    14.0  58.483       0   \n",
       "1       3      3  169357        1   34487.266323    14.0  27.978       1   \n",
       "2       4      4  149099        1   45864.738658    13.0  37.012       1   \n",
       "3       8      6  218371        1   23202.102210    14.0  22.001       1   \n",
       "4       8      7  232055        1       0.000000    12.0   3.585       0   \n",
       "\n",
       "   black  hispanic birth_month  birth_year  \n",
       "0      0         1        July        1982  \n",
       "1      0         1   September        1983  \n",
       "2      0         1    February        1981  \n",
       "3      0         1     January        1982  \n",
       "4      0         1       April        1983  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at first few rows of dataset\n",
    "nlsy97[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5aff70",
   "metadata": {},
   "source": [
    "### Part II: Predicting college attendance among those who have completed high school"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "826c44d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find subset of respondents who completed high school; drop any remaining units with item non-response\n",
    "HS_Mask = (nlsy97['yrssch']>=12)\n",
    "nlsy97 = nlsy97[HS_Mask]\n",
    "nlsy97 = nlsy97.dropna()\n",
    "\n",
    "# Create dummy variable for completion of a 4-year degree (by age 28)\n",
    "nlsy97['college'] = 1*(nlsy97['yrssch']>=16)\n",
    "\n",
    "# Create asvab test score bins\n",
    "# Define bins and labels\n",
    "bins = [0, 25, 50, 75, 100]\n",
    "labels = ['asvab_Q1', 'asvab_Q2', 'asvab_Q3', 'asvab_Q4']\n",
    "nlsy97['asvab_level'] = pd.cut(nlsy97['asvab'], bins=bins, labels=labels, right=False)\n",
    "\n",
    "# Add constant to the dataframe\n",
    "nlsy97['intercept']=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "921cff1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>asvab_level</th>\n",
       "      <th>asvab_Q1</th>\n",
       "      <th>asvab_Q2</th>\n",
       "      <th>asvab_Q3</th>\n",
       "      <th>asvab_Q4</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>college</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.905098</td>\n",
       "      <td>0.771341</td>\n",
       "      <td>0.582532</td>\n",
       "      <td>0.343358</td>\n",
       "      <td>0.656598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.094902</td>\n",
       "      <td>0.228659</td>\n",
       "      <td>0.417468</td>\n",
       "      <td>0.656642</td>\n",
       "      <td>0.343402</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "asvab_level  asvab_Q1  asvab_Q2  asvab_Q3  asvab_Q4       All\n",
       "college                                                      \n",
       "0            0.905098  0.771341  0.582532  0.343358  0.656598\n",
       "1            0.094902  0.228659  0.417468  0.656642  0.343402"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# College attendance by asvab quartile\n",
    "pd.crosstab(nlsy97['college'], nlsy97['asvab_level'], normalize='columns', margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2170f2aa",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Logistic Regression supports only penalties in ['l1', 'l2', 'elasticnet', 'none'], got None.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/xk/_7lm526x25v801z84vrr9yzc0000gn/T/ipykernel_26586/195289665.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlsy97\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'college'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msolver\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'newton-cg'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpenalty\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_intercept\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1089\u001b[0m         \u001b[0mThe\u001b[0m \u001b[0mSAGA\u001b[0m \u001b[0msolver\u001b[0m \u001b[0msupports\u001b[0m \u001b[0mboth\u001b[0m \u001b[0mfloat64\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfloat32\u001b[0m \u001b[0mbit\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1090\u001b[0m         \"\"\"\n\u001b[0;32m-> 1091\u001b[0;31m         \u001b[0msolver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_solver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msolver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpenalty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdual\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1092\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1093\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumbers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNumber\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mC\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\u001b[0m in \u001b[0;36m_check_solver\u001b[0;34m(solver, penalty, dual)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mall_penalties\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"l1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"l2\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"elasticnet\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"none\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpenalty\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_penalties\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m     56\u001b[0m             \u001b[0;34m\"Logistic Regression supports only penalties in %s, got %s.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mall_penalties\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpenalty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Logistic Regression supports only penalties in ['l1', 'l2', 'elasticnet', 'none'], got None."
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "X = nlsy97[['intercept', 'female', 'black', 'hispanic', 'asvab']]\n",
    "Y = nlsy97['college']\n",
    "\n",
    "model = LogisticRegression(solver='newton-cg', penalty=None, fit_intercept=False).fit(X,Y)\n",
    "model.coef_\n",
    "\n",
    "# Construct and display confusion matrix\n",
    "results = pd.DataFrame(model.coef_.reshape((-1,1)), index=X.columns.values, columns=['Coefficient'])\n",
    "print(\"Logistic Regression Results for College Attendence\")\n",
    "print(\"\")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88ea17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = confusion_matrix(nlsy97['college'], model.predict(X))\n",
    "\n",
    "# Row and column labels\n",
    "row_labels = ['College', 'High School']\n",
    "col_labels = ['College (Pred)', 'High School (Pred)']\n",
    "\n",
    "# Construct and display confusion matrix\n",
    "matrix = pd.DataFrame(matrix, index=row_labels, columns=col_labels)\n",
    "print(\"Confusion Matrix\")\n",
    "print(\"\")\n",
    "print(matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25a84dd",
   "metadata": {},
   "source": [
    "### Part III: Bootstrap inference on average partial effect (APE) of Asvab percentile score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09260908",
   "metadata": {},
   "outputs": [],
   "source": [
    "S = 1000                 # Number of bootstrap samples\n",
    "M = np.empty((S,2))      # Matrix with posterior draws of statistics of interest\n",
    "N = len(Y)               # Number of observations in dataset\n",
    "\n",
    "# Compute average partial effect of asvab score using sample-in-hand\n",
    "Xb = X @ model.coef_.T\n",
    "ape_hat_asvab = np.mean((np.exp(Xb)/(1+np.exp(Xb))**2)*model.coef_[0,-1])  \n",
    "\n",
    "print(\"\")\n",
    "print(f\"Logit estimate of the average partial effect of 1 asvab percentile: {ape_hat_asvab:.4f}\")\n",
    "\n",
    "# Perform s=1,...,S bootstrap simulations    \n",
    "for s in range (0,S):\n",
    "    bth_sample = nlsy97.sample(n=N, replace=True)  # Take bth bootstrap sample\n",
    "    X = bth_sample[['intercept', 'female', 'black', 'hispanic', 'asvab']]\n",
    "    Y = bth_sample['college']\n",
    "    model_b = LogisticRegression(solver='newton-cg', penalty=None, fit_intercept=False).fit(X,Y)\n",
    "    M[s,0] = model_b.coef_[0,-1]  \n",
    "    Xb = X @ model_b.coef_.T\n",
    "    M[s,1] = np.mean((np.exp(Xb)/(1+np.exp(Xb))**2)*model.coef_[0,-1])  \n",
    "    \n",
    "# Save Efron Bootstrap result in a dataframe\n",
    "EB=pd.DataFrame({'Asvab logit coef':M[:,0], 'Asvab APE':M[:,1]})\n",
    "\n",
    "print(\"\")\n",
    "print(EB.quantile([.025, .975]))\n",
    "\n",
    "# Construct percentile and reverse percentile confidence intervals\n",
    "lower_bnd = EB.quantile([.025, .975]).iloc[(0,1)]\n",
    "upper_bnd = EB.quantile([.025, .975]).iloc[(1,1)]\n",
    "print(\"\")\n",
    "print(f'A basic percentile boostrap confidence interval for asvab APE is: {lower_bnd:.4f} to {upper_bnd:.4f}')\n",
    "\n",
    "lower_bnd = 2*ape_hat_asvab - EB.quantile([.025, .975]).iloc[(1,1)]\n",
    "upper_bnd = 2*ape_hat_asvab - EB.quantile([.025, .975]).iloc[(0,1)]\n",
    "print(\"\")\n",
    "print(f'A reverse percentile boostrap confidence interval for asvab APE is: {lower_bnd:.4f} to {upper_bnd:.4f}')\n",
    "\n",
    "# Scatter (use seaborn add-on to matplotlib)\n",
    "sns.set_style(\"dark\", {'axes.linewidth': 2, 'axes.edgecolor':'black'})\n",
    "sns.jointplot(x=\"Asvab logit coef\",y=\"Asvab APE\", data=EB, kind=\"scatter\", \\\n",
    "              height=7, space=0.35, color=\"#003262\", xlim=(0.0325,0.0475), ylim=(0.0065,0.0075), marker='+',\n",
    "              marginal_ticks=True, marginal_kws=dict(bins=30, fill=True, color='#FDB515', linewidth=1))\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig(graphics + 'Figure_Reverse Percentile_Bootstrap_Asvab_APE.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e6ecbf",
   "metadata": {},
   "source": [
    "### Part IV: Assessing `balance'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46f32c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = nlsy97[['intercept', 'female', 'black', 'hispanic', 'asvab']]\n",
    "Y = nlsy97['college']\n",
    "\n",
    "model = LogisticRegression(solver='newton-cg', penalty=None, fit_intercept=False).fit(X,Y)\n",
    "model.coef_\n",
    "\n",
    "nlsy97['pscore'] = model.predict_proba(X)[:,1] # col 0 gives Pr(Y=0|X),  col 1 gives Pr(Y=1|X)\n",
    "ps_bins = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
    "labels = ['ps_D01', 'ps_D02', 'ps_D03','ps_D04','ps_D05','ps_D06','ps_D07','ps_D08','ps_D09','ps_D10']\n",
    "nlsy97['ps_level'] = pd.cut(nlsy97['pscore'], bins=ps_bins, labels=labels, right=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eff0592",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(nlsy97['college'], nlsy97['ps_level'], normalize=True, margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173d825b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create propensity score balance figure\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Plot the histograms\n",
    "ax.hist(nlsy97['pscore'].loc[(nlsy97['college'] == 0)], bins=ps_bins, alpha=0.5, label='High School', color='#003262')\n",
    "ax.hist(nlsy97['pscore'].loc[(nlsy97['college'] == 1)], bins=ps_bins, alpha=0.5, label='College', color='#FDB515')\n",
    "\n",
    "# Add labels and title\n",
    "ax.set_xlabel('Propensity Score')\n",
    "ax.set_ylabel('Number of Observations')\n",
    "ax.set_title('Balancing Plot')\n",
    "\n",
    "# Add legend\n",
    "ax.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
